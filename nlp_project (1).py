# -*- coding: utf-8 -*-
"""NLP_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hfrxjZKLO5BymLi0ShR6xDrXJoec_HLo
"""

#IMPORTING ALL THE NECESSARY LIBERARIES
import numpy as np
import pandas as pd
import os
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from google.colab import drive
drive.mount('/content/drive')

"""**LOADING DATA SET AND PREPROCESSING**"""

# Path to the dataset file
file_path = "/content/drive/MyDrive/mxm_dataset_train.txt"

# Print the first 10 lines of the dataset to verify structure
with open(file_path, 'r', encoding='utf-8') as f:
    for _ in range(10):
        print(f.readline().strip())

# Extract top words from the dataset
top_words = []
with open(file_path, 'r', encoding='utf-8') as f:
    for line in f:
        if line.startswith("%"):
            top_words = line[1:].split(",")  # Extract words after %
            break

print("Top words extracted:", top_words[:10])  # Print the first 10 words

##PARSING FIRST SONG DATA
song_data = []  # List to store parsed song data

with open(file_path, 'r', encoding='utf-8') as f:
    for line in f:
        # Skip comment and top words lines
        if line.startswith("#") or line.startswith("%"):
            continue
        # Parse song data
        parts = line.strip().split(",")
        track_id = parts[0]  # First field: track ID
        mxm_track_id = parts[1]  # Second field: musixmatch track ID
        word_counts = parts[2:]  # Remaining fields: sparse word counts
        lyrics = {}
        for wc in word_counts:
            # Ensure entry contains a colon
            if ":" in wc:
                idx, cnt = wc.split(":")
                lyrics[top_words[int(idx) - 1]] = int(cnt)  # Map index to word
        # Store the parsed data
        song_data.append({
            "track_id": track_id,
            "mxm_track_id": mxm_track_id,
            "lyrics": lyrics
        })

print("First song:", song_data[0])  # Print the first parsed song

#CONVERTING SPARSE LYRICS TO PLAIN TEXT:
for song in song_data:
    # Combine words and counts into plain text lyrics
    plain_lyrics = " ".join([f"{word} " * count for word, count in song["lyrics"].items()])
    song["plain_lyrics"] = plain_lyrics.strip()  # Add plain lyrics field

print("Plain lyrics for first song:", song_data[0]["plain_lyrics"])  # Check result

"""Pre-Processing"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import os
nltk.data.path.append("/usr/local/share/nltk_data")
nltk.download('punkt_tab')

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

# Preprocess lyrics
def preprocess_lyrics(lyrics):
    tokens = word_tokenize(lyrics.lower())  # Tokenize and lowercase
    filtered = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
    return " ".join(filtered)

# Apply preprocessing to each song
for song in song_data:
    song["processed_lyrics"] = preprocess_lyrics(song["plain_lyrics"])

print("Processed lyrics for first song:", song_data[0]["processed_lyrics"])  # Check result

"""**FEATURE EXTRACTION**"""

# Calculate word counts for each song
for song in song_data:
    song["word_count"] = sum(song["lyrics"].values())

# Summary statistics for word counts
import numpy as np
word_counts = [song["word_count"] for song in song_data]
print("Average word count:", np.mean(word_counts))
print("Median word count:", np.median(word_counts))
print("Max word count:", np.max(word_counts))
print("Min word count:", np.min(word_counts))

# Calculate unique vocabulary size
unique_words = set()
for song in song_data:
    unique_words.update(song["lyrics"].keys())

print("Total unique words in dataset:", len(unique_words))

"""**Generating Sentence Embeddings Using SBERT**"""

pip install sympy==1.11.1

pip install sentence-transformers

from sentence_transformers import SentenceTransformer

# Load SBERT model
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and efficient model

# Extract processed lyrics
lyrics_corpus = [song["processed_lyrics"] for song in song_data]

# Generate embeddings
embeddings = sbert_model.encode(lyrics_corpus, convert_to_tensor=True)

# Add embeddings to each song's data
for i, song in enumerate(song_data):
    song["embedding"] = embeddings[i]

print("Embedding for first song:", song_data[0]["embedding"])  # Check result

"""**Preparing Data for BM25**"""

pip install rank-bm25

from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

# Tokenize lyrics for BM25
tokenized_corpus = [word_tokenize(song["processed_lyrics"]) for song in song_data]

# Initialize BM25
bm25 = BM25Okapi(tokenized_corpus)

# Example: Query BM25
query = "love forever"
tokenized_query = word_tokenize(query.lower())
bm25_scores = bm25.get_scores(tokenized_query)

# Add BM25 score to each song (optional)
for i, song in enumerate(song_data):
    song["bm25_score"] = bm25_scores[i]

# Get top 5 results
top_results = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:5]
for idx in top_results:
    print(song_data[idx]["track_id"], song_data[idx]["processed_lyrics"])

"""**Training a Retrieval Model**"""

pip install datasets

from datasets import Dataset

pip install --upgrade datasets sentence-transformers

import os
os.environ["WANDB_MODE"] = "offline"

from sentence_transformers import InputExample, losses
from torch.utils.data import DataLoader
from datasets import Dataset  # Import Hugging Face Dataset

# Example labeled data (query, song, label)
train_samples = [
    InputExample(texts=["love me like you do", "love song lyrics"], label=1.0),
    InputExample(texts=["I hate everything", "happy love song"], label=0.0)
]

# DataLoader
train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=8)

# Loss function
train_loss = losses.CosineSimilarityLoss(model=sbert_model)

# Fine-tune model
sbert_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)

"""**Evaluating the Model**"""

from sklearn.metrics import precision_recall_fscore_support

# Example true and predicted labels
y_true = [1, 0, 1, 0, 1]  # True relevance labels
y_pred = [1, 0, 1, 0, 0]  # Predicted relevance labels

# Calculate metrics
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary")
print(f"Precision: {precision}, Recall: {recall}, F1-Score: {f1}")

"""**SAVING THE MODELS**

S-BERT MODEL
"""

from sentence_transformers import SentenceTransformer

# Assume your fine-tuned SBERT model is stored in sbert_model
sbert_model.save('/content/sbert_finetuned')  # Save the fine-tuned model

# Zip the model directory for download
!zip -r sbert_finetuned.zip /content/sbert_finetuned

"""Saving BM25 and Song Data"""

import pickle

# Save song_data without embeddings (just lyrics and track IDs)
for song in song_data:
    if "embedding" in song:  # Remove SBERT embeddings to reduce size
        del song["embedding"]

# Save the song data to a pickle file
with open('/content/song_data.pkl', 'wb') as f:
    pickle.dump(song_data, f)

# Zip the pickle file for download
!zip song_data.zip /content/song_data.pkl

import zipfile
import pickle
from sentence_transformers import SentenceTransformer

# Step 1: Extract the ZIP file
zip_path = '/content/song_data.zip'
extract_path = '/content/extracted_data'

print("Extracting ZIP file...")
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
print("File extracted successfully!")

# Step 2: Load the .pkl file
pkl_file_path = '/content/extracted_data/content/song_data.pkl'
print("Loading song_data...")
with open(pkl_file_path, 'rb') as f:
    song_data = pickle.load(f)
print("song_data loaded successfully!")

# Step 3: Load SBERT model
print("Loading SBERT model...")
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Step 4: Generate embeddings
print("Generating embeddings...")
lyrics_corpus = [song["processed_lyrics"] for song in song_data]
embeddings = sbert_model.encode(lyrics_corpus, convert_to_tensor=False)  # Convert to NumPy

# Add embeddings back to song_data
for i, song in enumerate(song_data):
    song["embedding"] = embeddings[i]

# Step 5: Save updated song_data
output_path = '/content/extracted_data/song_data_with_embeddings.pkl'
print("Saving song_data with embeddings...")
with open(output_path, 'wb') as f:
    pickle.dump(song_data, f)

print(f"Precomputation complete! Embeddings saved to {output_path}")

